===================
**Acknowledgment**
===================

We, the summer interns team of edX-Data Analytics, are overwhelmed in all humbleness
and gratefulness to acknowledge our depth to all those who have helped us to put our ideas
and assigned work, well above the level of simplicity and into something concrete.

We all thank whole heartedly, **Prof. Deepak Phatak** for selecting us a part of his valuable
project, constantly motivating for doing better and showing complete confidence in our work.

We are indebted to our Project Manager, **Mr. Parag Tiwari** for his constant help and support in
light of this project. We are very thankful to our mentors **Mr. Pushpak Burange**, **Ms. Mitali Nayak** and **Mr. Praveen Pal** for their valuable suggestions, guidance and encouragement. Also in the process, we learnt a lot other technical
and non technical things from them.

We would like to thank **Mr. Mayank Paliwal** and **Mr. Rahul Kharat** for making our stay here as summer
interns comfortable and for all their administrative help.

Finally we also like to thank all other colleagues working in different projects under Prof.
Phatak for helping us at small problems as well as critical junctures.

**Team Members**
----------------

Sachin Vilas Sable

 sablesachin3@gmail.com

Akansha

 akansha.nitrkl@gmail.com

B. Pallavi Reddy

 bpallavi.reddy9@gmail.com

Rounak Nandanwar

 rounak.nandanwar@gmail.com

Shubham Utwal

 shubhamutwal@gmail.com

 
**Mentors**
------------

Mr. Parag Tiwari-Project Manager at IITB

Mr. Pushpak Burange

Mr. Praveen Pal-Jr. Software Engineer at IITB

Ms. Mitali Nayak-Jr. Software Engineer at IITB

================
**Declaration**
================

We declare that this written submission represents our ideas in our own words and where
others' ideas or words have been included. We have adequately cited and referenced the
original sources. We also declare that we have adhered to all principles of academic honesty
and integrity and have not misrepresented or fabricated or falsified any idea/data/fact/source
in our submission. We understand that any violation of the above will be cause for
disciplinary action by the Institute and can also evoke penal action from the sources which
have thus not been properly cited or from whom proper permission has not been taken when
needed.



=============
**Abstract**
=============

IIT Bombay now has its contributions(in the form of two courses) to the prestigious edX platform. EdX is a massive open online course (MOOC) destination site and online learning platform that is open sourced, founded by the Massachusetts Institute of Technology and Harvard University. Though it is an ITS(Intelligent Tutoring System), students find out some or the other way to play around with the features of the system through which they reach the final(correct) answer without even thinking deeply about the question asked. So to avoid such instances, we need to determine those students who are gaming the system(behavior aimed at obtaining correct answers and advancing within the tutoring curriculum by systematically taking advantage of regularities in the software’s feedback and help). For this, we analyzed the log files generated by the student activities in the ITS (Intelligent tutoring system) i.e, the edX course site and extracted some features relevant to finding out the students who are gaming the system. Determination of such students will help us in the future to intervene the learning process, so as to prevent such students from  gaming and help them in learning the course.

We have also written queries to extract information which will be useful for data analytics. Using this data, the ITS designer and the course instructors can get useful information as to how well the students are giving positive response to the system and how popular and effecive their courses are. To aid them, a visualized interface has been designed so that it becomes easier and graphically legible to infer such aspects of a student's interest.  


=================
**INTRODUCTION**
=================

Data analytics is the discovery and communication of meaningful patterns in data. There is extensive use of mathematics and statistics, the use of descriptive techniques and predictive models to gain valuable knowledge from data - data analysis. The insights from data are used to recommend action or to guide decision making rooted in business context. Thus, analytics is not so much concerned with individual analyses or analysis steps, but with the entire methodology. From our project's point of view, we are concerned with 'EdX Analytics'. In this, we have to figure out the different patterns hidden in the log files generated. 

The log files have entries for every activity ever made by a student on the edX platform. Each and every click on tabs or question attempted or a video being played, all the entries are created in the log file. From this log file, we parse out the relevant information and find out patterns to detect that a student is showing any actions involving gaming the system. Gaming, as described above, is any behaviour aimed at obtaining correct answers and advancing within the tutoring curriculum by systematically taking advantage of regularities in the software’s feedback and help. Thus, classifying the gaming students is our priority. This involves careful feature extraction, data analytics and machine learning on the features for classification. 

Feedback to the ITS designer and course tutor is equally important. It is essential to know whether the system is doing a good work or not. This involves calculating a student's response based on various categories. A visualized form of such information is useful to them for deciding what reformations they need to apply, so as to make the course even more compelling and interesting to the students.

============
**PURPOSE**
============

The main purpose of our project is to determine which student is gaming the system so that we can apply interventions in their tutor system. For example if a question has some hint options, then a student can repeatedly use that hint option to reach to the correct answer. But, this kind of behaviour tampers the process of learning of the student. So, our main aim is to filter out such students who get involved in off-task behaviours and implement interventions only on those students' tutor system, which would passify their gaming process and indirectly motivate them to learn and attempt the quizes whole-heartedly. To implement such a system, relevant feature extraction from the log files is a necessity. Then comes the use of machine learning to classify a student as gaming or not gaming based on the features extracted.

Secondly, data analytics also involves a thorough study of the database containing all the demographic and activity information of a user. From this data, one can infer as to which category of students are mostly interested in learning the course. This requires queries to be written on that data from which we can extract relevant information. A visualized form of such information needs to be created as a feedback to the course designers. Looking at this, they can decide what reformations they need to apply, so as to make the course seem even more compelling and interesting to the students.


==========
**SCOPE**
==========

Implementing the ideas described above will make the tutoring system very efficient while grading a student. Any normal ITS, without any provisions for detection of a student who is gaming and implementing interventions in their system, will award a certificate to any student who has completed the course. But this would make a system very incompetent in correctly grading an undeserving student. Thus our idealogies would aid the ITS in fairly classifying the students into gaming and not gaming and accrordingly award the certificates to the deserving students. In addition, the tutoring system will be able to pick out the 'gaming' students and interevene their learning process so that their learning skills also match the regular students' learning, thereby widening the scope of the efficiency of the edX course site.

=======================
**Overall description**
=======================

EdX-data analyzer uses data genreated by edX  in the form of log entries and the database it creates. Data containing information related to the students is stored in the database 'edxapp'. Data from  edxapp is used for analysis purposes like number of dropouts according to education level, location, gender, number of students according to their eduation level enrolled in a particular course. 
Log entries genrated by the server will help us to find if the student is gaming the system or not. EdX-data analyzer will constantly look for the new entries  in the log files and parse, process  them to find wheather students are gaming or not.
EdX-data analyzer parses the log file entries and stores them using hive on hadoop  distributed file system.

=====================
**Product Functions**
=====================

EdX-data analyzer serves main purpose of determining wheather the student is gaming the system or not. It can be used to interrupt the student who are tring to game the system and adjust the tutor system such that it will be difficult for the student to game the system.

===============
**Constraints**
===============

Analysing the log data to find whether the student is gaming the system or not involves many constriants like diffculties in predicting the state of mind of person by just looking at it's interaction with the system. It is not possible to determine if a student is sleeping while watching the video or whether a student is paying proper attention. Along with difficulties in predicting the state of mind of the student, it is also not possible to note down each and every action or interaction of the user with his/her system due to privacy policies. Instance of this diffcutly can be a situation for example, a student who pauses the video might be pausing the video and indulging in the other off task behavior or a student might be getting confused while watching the video and now try to understand the concept over the internet. As seen in the example, it is difficult to predict the extact state mind of the student. To determine whether the student is gaming or not, detailed analysis is required. Even if we are able to determine whether the student is gaming or not,  we need to take some action to prevent the student from the gaming. But, question is how to determine which steps should be taken to stop the student from gaming as it will depend on the reason behind the gaming which is a furture part of analysis not covered here.

================================
**Assumptions and Dependancies**
================================

EdX-data analyzer assumes that log entries genrated by the EdX ITS server are error free. As only source of input to EdX-data analyzer is data provided by EdX ITS server. EdX-data analyzer totally depends on the EdX ITS for the data.


======================
**Technologies used:**
======================

1. Hadoop:
----------

Apache Hadoop is an open source software project that enables the distributed processing of large data sets across clusters of commodity servers. It is designed to scale up from a single server to thousands of machines, with a very high degree of fault tolerance. Rather than relying on high-end hardware, the resiliency of these clusters comes from the software’s ability to detect and handle failures at the application layer.

Apache Hadoop has two main subprojects:

MapReduce - The framework that understands and assigns work to the nodes in a cluster.
HDFS - A file system that spans all the nodes in a Hadoop cluster for data storage. It links together the file systems on many local nodes to make them into one big file system. HDFS assumes nodes will fail, so it achieves reliability by replicating data across multiple nodes

2. Hive:
--------


Hive is a runtime Hadoop support structure that allows anyone who is already fluent with SQL (which is commonplace for relational data-base developers) to leverage the Hadoop platform right out of the gate. 
Hive allows SQL developers to write Hive Query Language (HQL) statements that are similar to standard SQL statements. HQL is limited in the commands it understands, but it is still useful. HQL statements are broken down by the Hive service into MapReduce jobs and executed across a Hadoop cluster.


3. Sqoop:
---------

Sqoop is a command-line interface application for transferring data between relational databases and Hadoop. It supports incremental loads of a single table or a free form SQL query as well as saved jobs which can be run multiple times to import updates made to a database since the last import. Imports can also be used to populate tables in Hive or HBase. Exports can be used to put data from Hadoop into a relational database. 

=========================
**Specific Requirements**
=========================

3.1 External Interface Requirements:
3.1.1 Hardware Requirements:
1.

===========================
**Functional Requirements**
===========================

1. The system shall analyse the data based on various parameters of the student such as location,age group,gender etc.
2. The instructor shall be able to choose the comparison parameters and input valid entries to be queried. The instructor shall be able to input the subject for the data to be queried.
3. The data shall be represented in visual format to be understood by the instructor. The visuals formats may include pie-charts,bar charts, line charts etc.
4. The system shall parse the log data and store the parsed data into relevant event related tables.
5. The system shall extract the relevant and useful data from the parsed data.
6. The system shall tell whether a student is gaming a system or not. The system shall do this after analysing the various actions performed by the student while giving the test.

============================
**Performance Requirements**
============================

1. The edx analytics shall support in courses having large number of students (in thousands). There shall be minimal delay in retrieving the data.
2. The analysis shall be done on the data which has not been processed,i.e,only new data shall be considered for analysis. This would avoid the reading of unneccesary data again and again. This would be called as incremental implementation of queries.

==============================
**Non Functional Requirments**
==============================

1. The visual diagrams displaying the analysis with various parameters of the student shall be in a easy form so as to be understood by each and every instructor including those belonging to non-mathematical back-ground.

===========================
**Individual Contribution**
===========================

===============
**Log Parsing**
===============

Log files provides information about the event data that is delivered in data packages. Events are emitted by the server or the browser to capture information about interactions with the courseware and the Instructor Dashboard in the LMS, and are stored in JSON documents. In the data package, event data is delivered in a log file.The JSON documents that include event data are delivered in a machine-readable format that is difficult to read . A sample is the following :- ::


   {"agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)
   Chrome/30.0.1599.101 Safari/537.36", "context": {"course_id": "edx/AN101/2014_T1",
   "module": {"display_name": "Multiple Choice Questions"}, "org_id": "edx", "user_id":
   9999999}, "event": {"answers": {"i4x-edx-AN101-problem-a0effb954cca4759994f1ac9e9434bf4_2_1":
   "yellow", "i4x-edx-AN101-problem-a0effb954cca4759994f1ac9e9434bf4_4_1": ["choice_0", "choice_2"]},
   "attempts": 1, "correct_map": {"i4x-edx-AN101-problem-a0effb954cca4759994f1ac9e9434bf4_2_1":
   {"correctness": "incorrect", "hint": "", "hintmode": null, "msg": "", "npoints": null,
   "queuestate": null}, "i4x-edx-AN101-problem-a0effb954cca4759994f1ac9e9434bf4_4_1":
   {"correctness": "correct", "hint": "", "hintmode": null, "msg": "", "npoints": null,
   "queuestate": null}}, "grade": 2, "max_grade": 3, "problem_id": "i4x://edx/AN101/problem/
   a0effb954cca4759994f1ac9e9434bf4", "state": {"correct_map": {}, "done": null, "input_state":
   {"i4x-edx-AN101-problem-a0effb954cca4759994f1ac9e9434bf4_2_1": {}, "i4x-edx-AN101-problem-
   a0effb954cca4759994f1ac9e9434bf4_4_1": {}}, "seed": 1, "student_answers": {}}, "submission":
   {"i4x-edx-AN101-problem-a0effb954cca4759994f1ac9e9434bf4_2_1": {"answer": "yellow", "correct":
   false, "input_type": "optioninput", "question": "What color is the open ocean on a sunny day?",
   "response_type": "optionresponse", "variant": ""}, "i4x-edx-AN101-problem-
   a0effb954cca4759994f1ac9e9434bf4_4_1": {"answer": ["a piano", "a guitar"], "correct": true,
   "input_type": "checkboxgroup", "question": "Which of the following are musical instruments?",
   "response_type": "choiceresponse", "variant": ""}}, "success": "incorrect"}, "event_source":
   "server", "event_type": "problem_check", "host": "precise64", "ip": "NN.N.N.N", "page": "x_module",
   "time": 2014-03-03T16:19:05.584523+00:00", "username": "AAAAAAAAAA"}

We can use pretty print to see this information in a readable format which is shown as below :- ::

   {
      "agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36",
      "context": {
          "course_id": "edx/AN101/2014_T1",
          "module": {
              "display_name": "Multiple Choice Questions"
          },
          "org_id": "edx",
          "user_id": 9999999
      },
      "event": {
          "answers": {
              "i4x-edx-AN101-problem-a0effb954cca4759994f1ac9e9434bf4_2_1": "yellow",
              "i4x-edx-AN101-problem-a0effb954cca4759994f1ac9e9434bf4_4_1": [
                  "choice_0",
                  "choice_2"
              ]
          },
          "attempts": 1,
          "correct_map": {
              "i4x-edx-AN101-problem-a0effb954cca4759994f1ac9e9434bf4_2_1": {
                  "correctness": "incorrect",
                  "hint": "",
                  "hintmode": null,
                  "msg": "",
                  "npoints": null,
                  "queuestate": null
              },
              "i4x-edx-AN101-problem-a0effb954cca4759994f1ac9e9434bf4_4_1": {
                  "correctness": "correct",
                  "hint": "",
                  "hintmode": null,
                  "msg": "",
                  "npoints": null,
                  "queuestate": null
              }
          },
          "grade": 2,
          "max_grade": 3,
          "problem_id": "i4x://edx/AN101/problem/a0effb954cca4759994f1ac9e9434bf4",
          "state": {
              "correct_map": {},
              "done": null,
              "input_state": {
                  "i4x-edx-AN101-problem-a0effb954cca4759994f1ac9e9434bf4_2_1": {},
                  "i4x-edx-AN101-problem-a0effb954cca4759994f1ac9e9434bf4_4_1": {}
              },
              "seed": 1,
              "student_answers": {}
          },
          "submission": {
              "i4x-edx-AN101-problem-a0effb954cca4759994f1ac9e9434bf4_2_1": {
                  "answer": "yellow",
                  "correct": false,
                  "input_type": "optioninput",
                  "question": "What color is the open ocean on a sunny day?",
                  "response_type": "optionresponse",
                  "variant": ""
              },
              "i4x-edx-AN101-problem-a0effb954cca4759994f1ac9e9434bf4_4_1": {
                  "answer": [
                      "a piano",
                      "a guitar"
                  ],
                  "correct": true,
                  "input_type": "checkboxgroup",
                  "question": "Which of the following are musical instruments?",
                  "response_type": "choiceresponse",
                  "variant": ""
              }
          },
          "success": "incorrect"
      },
      "event_source": "server",
      "event_type": "problem_check",
      "host": "precise64",
      "ip": "NN.N.N.N",
      "page": "x_module",
      "time": "2014-03-03T16:19:05.584523+00:00",
      "username": "AAAAAAAAAA"
   }
   
   
Structure of log files
----------------------

There is an entry corresponding to every event generated in the log files.Different types of events are generated.Common fileds of those event are as follows:-
 

:Agent:
| Browser agent string of the user who triggered the event
:Context:
| For all events, this field includes member fields that identify:
   * The course_id of the course that generated the event. 
   * The org_id of the organization that lists the course. 
   * The user_id of the individual who is performing the action. 
:Event:
| This field includes member fields that identify specifics of each triggered event.
:event_source:
| Specifies whether the triggered event originated in the browser or on the server.
:event_type:
| The type of event triggered.
:Host:
| The site visited by the user, for example, courses.edx.org.
:ip:
| IP address of the user who triggered the event.
:page:
| The '$URL' of the page the user was visiting when the event was emitted.
:session:
| This 32-character value is a key that identifies the user's session.
:time:
| Gives the UTC time at which the event was emitted in 'YYYY-MM- DDThh:mm:ss.xxxxxx' format.
:username:
| The username of the user who caused the event to be emitted. This string is empty for anonymous events, such as when the user is not logged in.
     

| Each and every event has its own attributes. Various types of events generated which we have used are as follows:-

* Problem_check generated at two sources , browser and server.
* Problem_show
* load_video
* play_video
* speed_change_video
* seek_video
* pause_video  

Procedure
----------
| Entries of log files are parsed and then stored in database.
| For each log entry genral data like ip address, event_type, event_source is stored in table log.
| Data related to specific event_type is stored in corresponding tables like play_video, puase_video.

Following jar files are used in the the java program to parse the log files:-

* java-json.jar
* apache-logging-log4j.jar
* commons-httpclient-3.0.1.jar
* commons-logging-1.1.3.jar
* hadoop-core-1.2.0.jar
* hive-cli-0.13.0.jar
* hive-common-0.13.0.jar
* hive-exec-0.13.0.jar
* hive-jdbc-0.13.0.jar
* hive-metastore-0.13.0.jar
* hive-service-0.13.0.jar
* hive.txt
* libfb303-0.9.0.jar
* libthrift-0.9.0.jar
* log4j-1.2.16.jar
* slf4j-api-1.7.7.jar
* slf4j-jdk14-1.7.7.jar

Let the old_size be the size of the file which has been already parsed and whose value has been stored in the status table. Let new_size be the size of the log file at present.
There are three cases for the log files which are as folllows:-

| 1. old_size < new_size

It means that new entries has been added to the log files . The value of number of lines that have been parsed is stored in status table .That value is retrieved and those many lines skipped.Rest of the lines are parsed.Log entries are in the form of json .Folllowing is a snippet for this case:- ::


   public class LineParser {
   public int parseline(String line)
	{
		int success=0;
		Database db = new Database();
		JSONObject rootObject;
		try {
			rootObject = new JSONObject(line);
			Log log = new Log();
	        JSONObject context = rootObject.getJSONObject("context");
	        log.setCourse_id(context.get("course_id").toString();    											
	        log.setOrg_id(context.get("org_id").toString												
	        try
	        {
	        	log.setUser_id((Integer)context.get("user_id"));											
	        }
	        catch(Exception e)
	        {
	        	log.setUser_id(0);
	        }
	        try
	        {
	            JSONObject module = context.getJSONObject("module");
	            log.setModule(module.get("display_name").toString());											//3rd field
	        }
	        catch (Exception e)
	        {
	        	log.setModule("");	
	        }
	        log.setEvent_source(rootObject.get("event_source").toString());									//7th field
	        log.setEvent_type(rootObject.get("event_type").toString());											//6th and 8th field
	        log.setHost(rootObject.get("host").toString());													//9th field
	        log.setIp(rootObject.get("ip").toString());														//10th field
	        log.setPage(rootObject.get("page").toString());													//11th field
	        String time = rootObject.get("time").toString();
	        String time2=time.substring(0, 10);
	        time2=time2.concat(" ");
	        //system.out.println("********"+time2+"***********");
	        time2=time2.concat(time.substring(11,19));
	        log.setTime(time2);																				//12th field
	        log.setUsername(rootObject.get("username").toString());											//13th field
	        log.setEvent(rootObject.get("event").toString());
	        try
	        {
	        	log.setSession(rootObject.get("session").toString());
	        }
	        catch(Exception e)
	        {
	        	log.setSession("");
	        }
	        
	    	success = db.insertlogdata(log);
		}
		catch (JSONException e1)
		{
			e1.printStackTrace();
		}   
		return success;
	}
   }

| 2. old_size=new_size

| The log file has not changed and no new entries added.

| 3. old_size >new_size

| This means that the log file has been archived.So all those file whose modification time is greater than the time stored in the status table, which is the modification time of the log file just processed, are retrieved and extracted.
| It is possible that the current file which program was parsing is modified i.e., new log entries are added to the file and is now archived. So, we need to find the archived file corresponding to the file which was being parsed and parse and process if any new entries were added to the file.
| The file whose modification date is oldest among those files is the file which was archived while program was parsing it. Those new entries which were added are parsed with the help of values such as linesparsed and size of file stored in the status table.Rest of the files are processed from beginning to the end.

Following is a code snippet of the following case::


   public class  {
   	public void handlefile()
   	{
   		System.out.println("Entered into filehandler");
   		
   		File mydirectory = new File("/home/sachin/workspace/json/src/json/log");
   		
   		System.out.println(mydirectory.isDirectory());
   		
   		File names[] = mydirectory.listFiles();
   		Database db = new Database();
   		
   		//getting filelastmodified date
   		long lastmdate=db.getfilelastmodified();
   		
   		//creating arraylist to store new tar.gz files
   		ArrayList<File> left = new ArrayList<File>();
   		
   		//adding new files into the arraylist
   		for(int i=0;i<names.length;i++)
   		{
   			if(names[i].lastModified()>lastmdate&&names[i].toString().matches(".*gz$"))
   			{
   				left.add(names[i]);
   				System.out.println("got match: "+names[i].lastModified()+" "+names[i]);
   			}
   		}
   	
   		long table[][] = new long[left.size()][2];
   		
   		//intializing two dimensional array
   		for(int i=0;i<left.size();i++)
   		{
   			table[i][0] = i;
   			table[i][1] = left.get(i).lastModified();
   			System.out.println(table[i][0]+" "+table[i][1]);
   		}
   		
   		//sorting according to the date
   		for (int c = 0; c <  left.size(); c++) 
   		{
   		   for (int d = 0; d < left.size() - c - 1; d++) 
   		   {
   			   if(table[d][1] > table[d+1][1])
   			   {
   				   long swap = table[d][0];
   				   table[d][0]=table[d+1][0];
   				   table[d+1][0]=swap;
   				   swap = table[d][1];
   				   table[d][1]=table[d+1][1];
   				   table[d+1][1]=swap;
   			   }
   		   }
   		}
   		InputStream is=null;
   		boolean first=true;
   		String line;
   		LineParser lp = new LineParser();
   		for(int i=0;i<left.size();i++)
   		{
   			try
   			{
   				if(first)
   				{
   					
   					System.out.println("started reading file "+left.get((int) table[i][0]));
   					int linenum = db.getlinenum();
   					System.out.println("line count in file"+linenum);
   					is = new GZIPInputStream(new FileInputStream(left.get((int) table[i][0])));
   BufferedReader buffered = new BufferedReader(new InputStreamReader(is));
   					int j=0;
   					/
   					while(j<linenum)
   					{
   						if((line=buffered.readLine())!=null)
   						{
   							j++;
   							System.out.println("The value of the j is "+j);
   						}
   					}
   					while((line=buffered.readLine())!=null)
   					{
   						System.out.println(line);
   						if(line.startsWith("{"))
   						{
   							lp.parseline(line);
   						}
   					}
   					buffered.close();
   					first=false;
   					db.setfilelastmodified(left.get((int) table[i][0]).lastModified());
   				}
   				else
   				{
   
   					System.out.println("started reading file "+left.get((int) table[i][0]));
   					is = new GZIPInputStream(new FileInputStream(left.get((int) table[i][0])));
   					Reader decoder = new InputStreamReader(is, "UTF-8");
   					BufferedReader buffered = new BufferedReader(decoder);
   					while((line=buffered.readLine())!=null)
   					{
   						if(line.startsWith("{"))
   						{
   							lp.parseline(line);
   						}
   						System.out.println(line);
   					}
   					buffered.close();
   					db.setfilelastmodified(left.get((int) table[i][0]).lastModified());
   				}
   			}
   			catch(Exception e)
   			{
   				System.out.println("error while reading tar.gz files"+left.get((int) table[i][0]));
   			}
   		}
   		db.setsize(0);
   		db.insertlinenum(0);
   	}
   }

And the flowchart for the same is:
 
.. image:: diagram.png


-------------------------------------------------------------------------------------------------
Feature : To find number of minimum attempts to answer question correctly per user per question.
-------------------------------------------------------------------------------------------------

In order to find the difficulty level of the question, it is required to know in how many minimum number of attempts a student was able to answer the question correctly. Edx allows a user to answer a question any number of times even after the student has already given the correct answer. This is why we need to consider only the first attempt in which the student correctly answered the question. As analyzing the attempts after the user has already correctly answered the question is futile because student already knew the answer and those extra attempts will only mean revision for the student or that the student is just playing around with the system.

The java class written for this purpose is Feature_no_of_attempts. The following snippet fulfills the above described functionality :- ::

   statement.executeQuery("insert overwrite table temp select problem_id,username,min(attempts) from problem_check_server where log_id>0 and log_id<=2000 and correctness='correct'  group by problem_id,username");
			//statement.executeQuery("insert overwrite table temp select problem_id,username,min(attempts) from problem_check_server where log_id>0 and log_id<=1000 and correctness='correct'  group by problem_id,username");
			System.out.println("first and last "+start+" "+end);
			statement.executeQuery("insert into table attempts select p.problem_id,p.username,0 from temp p where not exists(select * from attempts where problem_id=p.problem_id and username=p.username)");
			resultset=statement.executeQuery("select * from temp");
			while(resultset.next())
			{
				a=resultset.getString(1);
				b=resultset.getString(2);
				c=resultset.getInt(3);
				statement.executeQuery("insert overwrite table attempts select problem_id,username,case when problem_id='"+a+"' and username='"+b+"' then "+c+" else attempts end as attempts from attempts");
				System.out.println(resultset.getString(1)+resultset.getString(2)+resultset.getString(3));
			}

Finding the number of minimum attempts per user per question has been done in three steps. This feature has been extracted using incremental approach. First step is to identify the new users from newly generated log entries and store them into temporary table along with the minimum number of attempts per question in the corresponding log entries. Second step is to insert dummy entries into the table attempts for the user not already present in the attempts table. Third step is to insert the values extracted in the temporary table into the attempts table.


------------------------------------------------------------------------------
Mapping Feature for machine learning : To determine level of activity per user
------------------------------------------------------------------------------

This feature is depending on the activity level of the user i. e., how much user is interacting with the system. It combines the results obtained after processing the log file and storing activity of user per day in table activity_per_day into a single value for per user. It is clear that students not interested in the course will have minimum activity level. also students who are trying to game the system will have high activity levels as they will constantly seek, pause videos frequently and while test they will answer the questions without contemplating over the questions.

The  java class written for this purpose is Feature_seek_time. The following snippet fulfills the above described functionality :- ::

   stmt.executeQuery("insert into table feature select apd.username,apd.course_id,0,0,0,0 from activity_per_day as apd where not exists (select username,course from feature as fe where fe.username=apd.username and fe.course=apd.course_id)");
   stmt.executeQuery("insert overwrite table tmp_feature_attempt select username,course_id,(10-abs((sum(video_act)-"+avg+")/("+avg+"*count(*))*10)) as value from activity_per_day group by username,course_id");
   stmt.executeQuery("insert overwrite table feature select f.username,f.course,f.f1,f.f2,case when f.username=apd.username and f.course=apd.course_id then apd.value else f.f3 end as f3,result from tmp_feature_attempt as apd join feature as f on apd.username=f.username and apd.course_id=f.course");

This is incremental query i.e., this will only process the log entries which were not processed earlier. To accomplish task of extracting feature three steps are required. First involves  inserting dummy entries for the entries which were added newly in the log table.Second step involves calculating level of activity and storing the values of activity level in the intermediate table. Value of activity is calculated such that users having level of activity at average level of all the users will be awarded highest score i.e., 10 and as students activity level deviate from the average value of activity level of all the users their score will decrease till the lowest possible score 10. Third step involved in which the scores which were calculated for each students will now be added into the table feature.












